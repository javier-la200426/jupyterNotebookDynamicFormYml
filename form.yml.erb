---

cluster: "pax"

bc_dynamic_js: true

attributes:
  conda_extensions: "0"

  extra_jupyter_args: ""

<%= File.read("/etc/ood/config/apps/dashboard/batch_connect/partials/num_cores_generic.yml").indent(2) %>
<%= File.read("/etc/ood/config/apps/dashboard/batch_connect/partials/num_memory_generic.yml").indent(2) %>
<%
require 'open3'
require 'set'

# Capture command output or return empty string on failure
def capture_stdout(cmd)
  stdout, status = Open3.capture2(cmd)
  status.success? ? stdout : ''
end

nodes_out = capture_stdout('scontrol show node --oneliner')

detected_types = Set.new
a100_sizes     = Set.new # values: 'a100_40', 'a100_80'

nodes_out.each_line do |line|
  # Gres=gpu:<type>:<count>
  if (m = line.match(/\bGres=gpu:(\w+)/))
    type = m[1]
    detected_types << type if type && type != 'null'
  end

  # Features or Feature may contain capacity hints like a100-40G / a100-80G
  feats = (line[/\bFeature(?:s)?=(\S+)/, 1] || '')
  a100_sizes << 'a100_40' if feats.include?('a100-40G')
  a100_sizes << 'a100_80' if feats.include?('a100-80G')
end

# If we know specific A100 sizes, prefer those over a generic 'a100'
if !a100_sizes.empty?
  detected_types.delete('a100')
end

# Human-friendly labels for known GPU types
label_map = {
  'h200'        => 'H200 - 140GB',
  'h100'        => 'H100 - 80GB',
  'a100'        => 'A100',
  'a100_40'     => 'A100 - 40GB',
  'a100_80'     => 'A100 - 80GB',
  'l40s'        => 'L40S - 48GB',
  'l40'         => 'L40 - 48GB',
  'rtx_6000ada' => 'RTX 6000Ada - 48GB',
  'rtx_a6000'   => 'RTX A6000 - 48GB',
  'rtx_6000'    => 'RTX 6000 - 24GB',
  'v100'        => 'v100 - 16GB',
  'p100'        => 'P100 - 16GB',
  't4'          => 'T4 - 16GB'
}

# Preferred display order (others fall to the end)
order = %w[h200 h100 a100_80 a100_40 a100 l40s l40 rtx_6000ada rtx_a6000 rtx_6000 v100 p100 t4]
priority = order.each_with_index.to_h

options = []
options << ['none', 'none']
options << ['any',  'any']

# Add A100 size-specific entries first if discovered
options << [label_map['a100_40'], 'a100_40'] if a100_sizes.include?('a100_40')
options << [label_map['a100_80'], 'a100_80'] if a100_sizes.include?('a100_80')

# Then add remaining detected types
remaining = detected_types.to_a
remaining.sort_by! { |t| priority.fetch(t, 999) }
remaining.each do |t|
  next if %w[a100_40 a100_80].include?(t) # already added above
  label = label_map[t] || t.upcase
  options << [label, t]
end
%>
  gpu_type:
    widget: "select"
    label: "GPU architecture"
    help: "Auto-detected from Slurm; choose 'any' if you don't have a preference."
    options:
<% options.each do |label, value| %>
      - [ "<%= label %>", "<%= value %>" ]
<% end %>
<%= File.read("/etc/ood/config/apps/dashboard/batch_connect/partials/all_partition.yml").indent(2) %>
    
  working_directory:
    widget: "text_field"
    label: "Top working directory"
    help: Top working directory for your Jupyter session to access your notebooks and data. Default is your **$HOME**.
    
  mode:
    widget: "radio"
    label: "Jupyter Lab or Notebook?"
    value: "1"
    options:
      - ["Jupyter Lab", "1"]
      - ["Jupyter Notebook", "0"]

  module:
    widget: text_field
    label: "Load supporting modules"
    help: ENTER module name **ONLY**. e.g. for "module load cuda/12.9" in command line, ENTER **cuda/12.2**


form:
  - conda_extensions
  - extra_jupyter_args
  - mode
  - bc_num_hours
  - num_cores
  - num_memory
  - partition
  - gpu_type
  - working_directory
  - module

